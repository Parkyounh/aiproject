
import os
import cv2
import torch
import numpy as np
import faiss
# CLIP ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import clip 
from PIL import Image

# SAM ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê°ì²´ ë§ˆìŠ¤í¬ë¥¼ ìœ„í•´ ìœ ì§€í•©ë‹ˆë‹¤.
from segment_anything import sam_model_registry, SamPredictor

# ==========================
# 1. SAM ë° CLIP ëª¨ë¸ ë¡œë“œ
# ==========================
# SAM ì„¤ì • (ê°ì²´ ë§ˆìŠ¤í¬ ì¶”ì¶œ ìš©ë„)
sam_checkpoint = "weights/sam_vit_h_4b8939.pth" # ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

# SAM ëª¨ë¸ ë¡œë“œ
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
sam_predictor = SamPredictor(sam) # ë³€ìˆ˜ ì´ë¦„ ë³€ê²½

# ğŸš¨ğŸš¨ğŸš¨ CLIP ëª¨ë¸ ë¡œë“œ ğŸš¨ğŸš¨ğŸš¨
# ViT-B/32ëŠ” ì¼ë°˜ì ì¸ ì„ íƒì…ë‹ˆë‹¤. ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì›í•˜ë©´ ViT-L/14 ë“±ì„ ì‚¬ìš©í•˜ì„¸ìš”.
clip_model, preprocess = clip.load("ViT-B/32", device=device)
clip_model.eval() 

def get_clip_embedding_from_masked_object(image_path):
    # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° SAM ë§ˆìŠ¤í¬ ì¶”ì¶œ
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        raise FileNotFoundError(f"Image not found at {image_path}")
        
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    
    # SAMì—ê²Œ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì „ë‹¬
    sam_predictor.set_image(image_rgb)
    
    # ğŸŸ¢ğŸŸ¢ğŸŸ¢ í”„ë¡¬í”„íŠ¸ ë³€ê²½: ë°”ìš´ë”© ë°•ìŠ¤ ì‚¬ìš© ğŸŸ¢ğŸŸ¢ğŸŸ¢
    H, W, _ = image_rgb.shape
    # ì¤‘ì•™ 80% ì˜ì—­ì„ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ì„¤ì • (ì£¼ ê°ì²´ê°€ ì¤‘ì•™ì— ìˆë‹¤ê³  ê°€ì •)
    margin_ratio = 0.1 # ìƒí•˜ì¢Œìš° 10% ì—¬ë°±
    x_min = int(W * margin_ratio)
    y_min = int(H * margin_ratio)
    x_max = int(W * (1 - margin_ratio))
    y_max = int(H * (1 - margin_ratio))
    
    input_box = np.array([[x_min, y_min, x_max, y_max]]) # [x_min, y_min, x_max, y_max]
    print(f"    -> SAM Bounding Box Prompt: {input_box[0]}")
    # ğŸŸ¢ğŸŸ¢ğŸŸ¢ ë³€ê²½ ë ğŸŸ¢ğŸŸ¢ğŸŸ¢

    # ë§ˆìŠ¤í¬ ê³„ì‚° (ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©)
    masks, scores, logits = sam_predictor.predict(
        point_coords=None,       # í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì•ˆ í•¨
        point_labels=None,       # í¬ì¸íŠ¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì•ˆ í•¨
        box=input_box,           # ë°”ìš´ë”© ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        multimask_output=False, # ê°€ì¥ ì¢‹ì€ ë§ˆìŠ¤í¬ í•˜ë‚˜ë§Œ ì„ íƒ
    )
    
    if masks is None or not masks.any():
        print(f"âš ï¸ Warning: No main object mask found for {image_path}. Using full image.")
        mask = np.ones((H, W), dtype=bool)
    else:
        # ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ë§ˆìŠ¤í¬ ì„ íƒ
        print(f"    -> SAM Mask Score: {scores[0]:.4f}")
        mask = masks[0] 

    # 2. ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ ì œê±° (ê²€ì€ìƒ‰ìœ¼ë¡œ ì±„ìš°ê¸°)
    # CLIPì€ íˆ¬ëª…ë„(Alpha) ì±„ë„ì„ ì˜ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ, ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    masked_image_rgb = image_rgb * mask[:, :, np.newaxis]
    
    # ğŸš¨ğŸš¨ğŸš¨ ê°ì²´ë§Œ ë‚¨ê¸´ ì´ë¯¸ì§€ ì €ì¥ ë¡œì§ (ì´ì „ ìš”ì²­ìœ¼ë¡œ ì¶”ê°€ë¨) ğŸš¨ğŸš¨ğŸš¨
    save_dir = "masked_images" # ì €ì¥í•  í´ë” ì´ë¦„
    os.makedirs(save_dir, exist_ok=True)
    
    # ì›ë³¸ íŒŒì¼ëª…ì—ì„œ í™•ì¥ìë¥¼ .pngë¡œ ë³€ê²½í•˜ì—¬ ì €ì¥
    base_name = os.path.basename(image_path)
    file_name_without_ext = os.path.splitext(base_name)[0]
    save_path = os.path.join(save_dir, f"{file_name_without_ext}_masked.png")
    
    # RGBë¥¼ BGRë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (OpenCV ê¸°ë³¸ í¬ë§·)
    masked_image_bgr = cv2.cvtColor(masked_image_rgb, cv2.COLOR_RGB2BGR)
    cv2.imwrite(save_path, masked_image_bgr)
    print(f"    ğŸ’¾ Masked image saved to: {save_path}")
    # ğŸš¨ğŸš¨ğŸš¨ ì €ì¥ ë¡œì§ ë ğŸš¨ğŸš¨ğŸš¨
    
    # 3. CLIP ì„ë² ë”© ì¶”ì¶œ
    # NumPy ë°°ì—´ì„ PIL Imageë¡œ ë³€í™˜í•˜ê³  CLIP ì „ì²˜ë¦¬ ì ìš©
    image_pil = Image.fromarray(masked_image_rgb)
    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)
    
    # CLIP ì„ë² ë”© ê³„ì‚°
    with torch.no_grad():
        embedding = clip_model.encode_image(image_tensor)
        
    # ë²¡í„° ì •ê·œí™”ëŠ” CLIP ì¸ì½”ë” ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ìµœì¢…ì ìœ¼ë¡œ í•œ ë²ˆ ë” L2 ì •ê·œí™”í•©ë‹ˆë‹¤.
    embedding_np = embedding.cpu().numpy().astype("float32")
    embedding_norm = embedding_np / np.linalg.norm(embedding_np)
    
    return embedding_norm.flatten()


# ==========================
# 3. ì´ë¯¸ì§€ í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ì¶œ
# ==========================
image_dir = "images"
image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) 
               if f.lower().endswith((".png", ".jpg", ".jpeg", ".webp"))]

if len(image_paths) < 2:
    raise ValueError("âŒ ë¹„êµí•  ì´ë¯¸ì§€ê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤!")

embeddings = []
for path in image_paths:
    print(f"ğŸ”¹ Processing: {path}")
    # ìˆ˜ì •ëœ CLIP ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©
    emb = get_clip_embedding_from_masked_object(path) 
    embeddings.append(emb)

embeddings = np.array(embeddings).astype("float32")


# ==========================
# 4. Faiss ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•œ IndexFlatIP ì‚¬ìš©)
# ==========================
dimension = embeddings.shape[1]
# ğŸš¨ğŸš¨ğŸš¨ IndexFlatIP (ë‚´ì ) ì‚¬ìš©: ì •ê·œí™”ëœ ë²¡í„°ì˜ ë‚´ì ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ê°™ìŠµë‹ˆë‹¤. ğŸš¨ğŸš¨ğŸš¨
index = faiss.IndexFlatIP(dimension) 
index.add(embeddings)
print(f"âœ… FAISS index built with {len(embeddings)} images using CLIP and IP Index.")


# ==========================
# 5. ìœ ì‚¬ë„ ê²€ìƒ‰ (ì˜ˆ: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê¸°ì¤€)
# ==========================
query_idx = 0
query_vector = embeddings[query_idx].reshape(1, -1)

# k=5ë¡œ ë³€ê²½í•˜ì—¬ ë” ë§ì€ ìœ ì‚¬ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
distances, indices = index.search(query_vector, k=5) 

print("\nğŸ” Query Image:", image_paths[query_idx])
print("ğŸ“¸ Most similar images:")

# IndexFlatIPë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— 'distances'ê°€ ê³§ 'similarity' ê°’ì…ë‹ˆë‹¤ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬).
for rank, idx in enumerate(indices[0]): 
    similarity = distances[0][rank]
    
    # ì²« ë²ˆì§¸ ê²°ê³¼ëŠ” ì¿¼ë¦¬ ì´ë¯¸ì§€ ìì‹ ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    if rank == 0:
        print(f"â­ Query itself: {image_paths[idx]} (similarity: {similarity:.4f})")
    else:
        print(f"{rank}. {image_paths[idx]} (similarity: {similarity:.4f})")