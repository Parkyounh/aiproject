import os
import cv2
import torch
import numpy as np
import faiss
# CLIP ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
import clip 
import sys
from PIL import Image

# ğŸš¨ğŸš¨ğŸš¨ rembg ë¼ì´ë¸ŒëŸ¬ë¦¬ import ğŸš¨ğŸš¨ğŸš¨
try:
    from rembg import remove
except ImportError:
    print("ğŸš¨ ì˜¤ë¥˜: rembg ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install rembg'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit()

# -----------------------------------------------------------
# 1. SAM ê´€ë ¨ ì½”ë“œ ì œê±° ë° CLIP ëª¨ë¸ ë¡œë“œ
# -----------------------------------------------------------

# SAM ëª¨ë¸ ë¡œë“œ ë° SamPredictor ê´€ë ¨ ì½”ë“œëŠ” ëª¨ë‘ ì œê±°í–ˆìŠµë‹ˆë‹¤.
device = "cuda" if torch.cuda.is_available() else "cpu"

# CLIP ëª¨ë¸ ë¡œë“œ (ë™ì¼)
clip_model, preprocess = clip.load("ViT-B/32", device=device)
clip_model.eval() 
print(f"âœ… CLIP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. (Device: {device})")


def get_clip_embedding_from_masked_object(image_path):
    """
    1. rembg ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë°°ê²½ì„ ì œê±°í•©ë‹ˆë‹¤.
    2. ë°°ê²½ì´ ì œê±°ëœ ì´ë¯¸ì§€ (íˆ¬ëª… ë°°ê²½)ë¥¼ ê²€ì€ìƒ‰ ë°°ê²½ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CLIPì— ì…ë ¥í•©ë‹ˆë‹¤.
    3. CLIP ì„ë² ë”©ì„ ì¶”ì¶œí•˜ê³  L2 ì •ê·œí™”í•©ë‹ˆë‹¤.
    """
    
    # 1. ì´ë¯¸ì§€ ë¡œë“œ (PIL Image ì‚¬ìš©)
    try:
        input_image_pil = Image.open(image_path).convert("RGB")
    except FileNotFoundError:
        raise FileNotFoundError(f"Image not found at {image_path}")
        
    # 2. ğŸš¨ğŸš¨ğŸš¨ rembgë¥¼ ì‚¬ìš©í•œ ë°°ê²½ ì œê±° ğŸš¨ğŸš¨ğŸš¨
    # rembg.remove()ëŠ” ë°°ê²½ì´ íˆ¬ëª…í•œ RGBA PIL Imageë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    # U2Net ë“±ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    print("    -> rembgë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ ì œê±° ì¤‘...")
    output_image_rgba = remove(input_image_pil)
    
    # 3. íˆ¬ëª… ë°°ê²½ (RGBA) ì´ë¯¸ì§€ë¥¼ ê²€ì€ìƒ‰ ë°°ê²½ (RGB) ì´ë¯¸ì§€ë¡œ ë³€í™˜
    # CLIPì€ íˆ¬ëª…ë„(Alpha) ì±„ë„ì„ ì˜ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ, ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    # a. Alpha ì±„ë„ ì¶”ì¶œ
    alpha_channel = output_image_rgba.split()[-1]
    
    # b. RGB ì±„ë„ê³¼ ë§ˆìŠ¤í¬ ê²°í•©
    masked_image_rgb = Image.new('RGB', output_image_rgba.size, (0, 0, 0)) # ê²€ì€ìƒ‰ ë°°ê²½
    masked_image_rgb.paste(output_image_rgba, mask=alpha_channel)
    
    # ğŸš¨ğŸš¨ğŸš¨ ê°ì²´ë§Œ ë‚¨ê¸´ ì´ë¯¸ì§€ ì €ì¥ ë¡œì§ (íˆ¬ëª… ë°°ê²½ ì²˜ë¦¬ í›„) ğŸš¨ğŸš¨ğŸš¨
    save_dir = "masked_images_rembg" # í´ë” ì´ë¦„ ë³€ê²½
    os.makedirs(save_dir, exist_ok=True)
    base_name = os.path.basename(image_path)
    file_name_without_ext = os.path.splitext(base_name)[0]
    # RGBA ê²°ê³¼ë¬¼ì„ ì €ì¥í•˜ì—¬ íˆ¬ëª… ë°°ê²½ì„ í™•ì¸í•˜ê±°ë‚˜, RGB ê²°ê³¼ë¬¼ì„ ì €ì¥ (ì—¬ê¸°ì„œëŠ” RGB)
    save_path = os.path.join(save_dir, f"{file_name_without_ext}_masked.png")
    
    # OpenCVë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  PILë¡œ ë°”ë¡œ ì €ì¥
    masked_image_rgb.save(save_path) 
    print(f"    ğŸ’¾ Masked image saved to: {save_path}")
    # ğŸš¨ğŸš¨ğŸš¨ ì €ì¥ ë¡œì§ ë ğŸš¨ğŸš¨ğŸš¨
    
    # 4. CLIP ì„ë² ë”© ì¶”ì¶œ
    image_tensor = preprocess(masked_image_rgb).unsqueeze(0).to(device)
    
    with torch.no_grad():
        embedding = clip_model.encode_image(image_tensor)
        
    # ë²¡í„° ì •ê·œí™”
    embedding_np = embedding.cpu().numpy().astype("float32")
    embedding_norm = embedding_np / np.linalg.norm(embedding_np)
    
    return embedding_norm.flatten()


# ==========================
# 3. ì´ë¯¸ì§€ í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ì¶œ (ë™ì¼)
# ==========================
image_dir = "images"
image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) 
               if f.lower().endswith((".png", ".jpg", ".jpeg", ".webp"))]

if len(image_paths) < 2:
    raise ValueError("âŒ ë¹„êµí•  ì´ë¯¸ì§€ê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤!")

embeddings = []
for path in image_paths:
    print(f"\nğŸ”¹ Processing: {path}")
    emb = get_clip_embedding_from_masked_object(path) 
    embeddings.append(emb)

embeddings = np.array(embeddings).astype("float32")
print(f"\nâœ… All embeddings extracted. Total images: {len(embeddings)}")


# ==========================
# 4. Faiss ì¸ë±ìŠ¤ ìƒì„± ë° ê²€ìƒ‰ (ë™ì¼)
# ==========================
dimension = embeddings.shape[1]
index = faiss.IndexFlatIP(dimension) 
index.add(embeddings)
print(f"âœ… FAISS index built with {len(embeddings)} images using CLIP and IP Index.")

query_idx = 0
query_vector = embeddings[query_idx].reshape(1, -1)
distances, indices = index.search(query_vector, k=5) 

print("\n" + "=" * 50)
print(f"âœ¨ ìµœì¢… ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ (rembg ê°ì²´ + CLIP ì„ë² ë”©)")
print("=" * 50)
print(f"ğŸ” Query Image: {image_paths[query_idx]}")
print("ğŸ“¸ Most similar images:")

for rank, idx in enumerate(indices[0]): 
    similarity = distances[0][rank]
    
    if rank == 0:
        print(f"â­ Query itself: {image_paths[idx]} (similarity: {similarity:.4f})")
    else:
        print(f"{rank}. {image_paths[idx]} (similarity: {similarity:.4f})")
print("=" * 50)
