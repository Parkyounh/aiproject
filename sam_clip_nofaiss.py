import os
import cv2
import torch
import numpy as np
import clip 
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì— ì‚¬ìš©

# SAM ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)
from segment_anything import sam_model_registry, SamPredictor

# ==========================
# 1. SAM ë° CLIP ëª¨ë¸ ë¡œë“œ (Faiss ì‚¬ìš© ì½”ë“œì™€ ë™ì¼)
# ==========================
sam_checkpoint = "weights/sam_vit_h_4b8939.pth" 
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
sam_predictor = SamPredictor(sam)

clip_model, preprocess = clip.load("ViT-B/32", device=device)
clip_model.eval() 

# ì´ì „ ì½”ë“œì˜ get_clip_embedding_from_masked_object í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# (ì´ í•¨ìˆ˜ëŠ” ì„ë² ë”© ì¶”ì¶œ ë° L2 ì •ê·œí™”ê¹Œì§€ ì™„ë£Œëœ ë²¡í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.)
def get_clip_embedding_from_masked_object(image_path):
    # (ì½”ë“œëŠ” Faiss ì‚¬ìš© ì½”ë“œì™€ ì™„ì „íˆ ë™ì¼í•©ë‹ˆë‹¤. ì¤‘ë³µì„ í”¼í•˜ê¸° ìœ„í•´ ìƒëµ)
    # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° SAM ë§ˆìŠ¤í¬ ì¶”ì¶œ...
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        raise FileNotFoundError(f"Image not found at {image_path}")
        
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    sam_predictor.set_image(image_rgb)
    
    H, W, _ = image_rgb.shape
    margin_ratio = 0.1
    x_min = int(W * margin_ratio); y_min = int(H * margin_ratio)
    x_max = int(W * (1 - margin_ratio)); y_max = int(H * (1 - margin_ratio))
    input_box = np.array([[x_min, y_min, x_max, y_max]])
    print(f"    -> SAM Bounding Box Prompt: {input_box[0]}")

    masks, scores, _ = sam_predictor.predict(
        point_coords=None, point_labels=None, box=input_box, multimask_output=False
    )
    
    if masks is None or not masks.any():
        print(f"âš ï¸ Warning: No main object mask found for {image_path}. Using full image.")
        mask = np.ones((H, W), dtype=bool)
    else:
        print(f"    -> SAM Mask Score: {scores[0]:.4f}")
        mask = masks[0]
        
    masked_image_rgb = image_rgb * mask[:, :, np.newaxis]
    
    # ì´ë¯¸ì§€ ì €ì¥ (masked_images í´ë”)
    save_dir = "masked_images"; os.makedirs(save_dir, exist_ok=True)
    base_name = os.path.basename(image_path)
    file_name_without_ext = os.path.splitext(base_name)[0]
    save_path = os.path.join(save_dir, f"{file_name_without_ext}_masked.png")
    masked_image_bgr = cv2.cvtColor(masked_image_rgb, cv2.COLOR_RGB2BGR)
    cv2.imwrite(save_path, masked_image_bgr)
    print(f"    ğŸ’¾ Masked image saved to: {save_path}")
    
    image_pil = Image.fromarray(masked_image_rgb)
    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)
    
    with torch.no_grad():
        embedding = clip_model.encode_image(image_tensor)
        
    embedding_np = embedding.cpu().numpy().astype("float32")
    # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ í•„ìˆ˜)
    embedding_norm = embedding_np / np.linalg.norm(embedding_np) 
    
    return embedding_norm.flatten()


# ==========================
# 2. ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ì¶œ (Faiss ì‚¬ìš© ì½”ë“œì™€ ë™ì¼)
# ==========================
image_dir = "images"
image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) 
               if f.lower().endswith((".png", ".jpg", ".jpeg", ".webp"))]

if len(image_paths) < 2:
    raise ValueError("âŒ ë¹„êµí•  ì´ë¯¸ì§€ê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤!")

embeddings = []
for path in image_paths:
    print(f"ğŸ”¹ Processing: {path}")
    emb = get_clip_embedding_from_masked_object(path) 
    embeddings.append(emb)

embeddings = np.array(embeddings).astype("float32")
print(f"âœ… All embeddings extracted. Total images: {len(embeddings)}")


# ==========================
# 3. ğŸš¨ğŸš¨ğŸš¨ Faissë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìœ ì‚¬ë„ ê²€ìƒ‰ ğŸš¨ğŸš¨ğŸš¨
# ==========================
query_idx = 0
query_vector = embeddings[query_idx].reshape(1, -1)
k = 5

# NumPyë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ë²¡í„°ì™€ ëª¨ë“  ì„ë² ë”© ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í•œ ë²ˆì— ê³„ì‚°
# L2 ì •ê·œí™”ëœ ë²¡í„°ë¥¼ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ, í–‰ë ¬ ê³±(ë‚´ì ) ê²°ê³¼ê°€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì…ë‹ˆë‹¤.
similarities = embeddings.dot(query_vector.T).flatten()

# ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ (ìœ ì‚¬ë„, ì¸ë±ìŠ¤) ìŒìœ¼ë¡œ ë¬¶ì–´ ì •ë ¬
sorted_indices = np.argsort(similarities)[::-1] # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬

# ìƒìœ„ kê°œ ê²°ê³¼ë§Œ ì¶”ì¶œ
top_k_indices = sorted_indices[:k]
top_k_similarities = similarities[top_k_indices]


print("\n" + "=" * 50)
print("âœ¨ Faiss ì—†ì´ NumPyë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼")
print(f"ğŸ” Query Image: {image_paths[query_idx]}")
print("ğŸ“¸ Most similar images:")
print("=" * 50)

for rank, idx in enumerate(top_k_indices):
    similarity = top_k_similarities[rank]
    
    if rank == 0:
        print(f"â­ Query itself: {image_paths[idx]} (similarity: {similarity:.4f})")
    else:
        print(f"{rank}. {image_paths[idx]} (similarity: {similarity:.4f})")

print("=" * 50)