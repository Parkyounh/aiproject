import os
import cv2
import torch
import numpy as np
import faiss
# CLIP ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê±°
from PIL import Image
from sklearn.preprocessing import normalize # ë²¡í„° ì •ê·œí™”ì— í•„ìš”

# SAM ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê°ì²´ ë§ˆìŠ¤í¬ì™€ ì„ë² ë”©ì„ ìœ„í•´ ìœ ì§€í•©ë‹ˆë‹¤.
from segment_anything import sam_model_registry, SamPredictor


# ==========================
# 1. SAM ëª¨ë¸ ë¡œë“œ
# ==========================
# SAM ì„¤ì • (ê°ì²´ ë§ˆìŠ¤í¬ ë° ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ìš©ë„)
sam_checkpoint = "weights/sam_vit_h_4b8939.pth" # ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”
model_type = "vit_h"
device = "cuda" if torch.cuda.is_available() else "cpu" # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

# SAM ëª¨ë¸ ë¡œë“œ
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
sam_predictor = SamPredictor(sam) # ë³€ìˆ˜ ì´ë¦„ ë³€ê²½


# ==========================
# 2. ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± í•¨ìˆ˜ (SAM ì„ë² ë”© ì‚¬ìš©, ë§ˆìŠ¤í¬ ì ìš©)
# ==========================
# ğŸš¨ SAMì˜ ì„ë² ë”© ì¶”ì¶œ ë°©ì‹ì„ í™œìš©í•˜ë„ë¡ í•¨ìˆ˜ ë¡œì§ì„ ë³€ê²½í•©ë‹ˆë‹¤.
def get_sam_embedding_from_masked_object(image_path):
    # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° SAM ë§ˆìŠ¤í¬ ì¶”ì¶œ
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        raise FileNotFoundError(f"Image not found at {image_path}")
        
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    
    # SAMì—ê²Œ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì „ë‹¬í•˜ê³ , ì „ì²´ ì´ë¯¸ì§€ ì„ë² ë”© ê³„ì‚° (ë§ˆìŠ¤í¬ì™€ ë¬´ê´€)
    sam_predictor.set_image(image_rgb)
    
    # ğŸŸ¢ğŸŸ¢ğŸŸ¢ ë°”ìš´ë”© ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì¤‘ì•™ 80% ì˜ì—­) ğŸŸ¢ğŸŸ¢ğŸŸ¢
    H, W, _ = image_rgb.shape
    margin_ratio = 0.1 # ìƒí•˜ì¢Œìš° 10% ì—¬ë°±
    x_min = int(W * margin_ratio)
    y_min = int(H * margin_ratio)
    x_max = int(W * (1 - margin_ratio))
    y_max = int(H * (1 - margin_ratio))
    
    input_box = np.array([[x_min, y_min, x_max, y_max]]) # [x_min, y_min, x_max, y_max]
    print(f"     -> SAM Bounding Box Prompt: {input_box[0]}")
    
    # ë§ˆìŠ¤í¬ ê³„ì‚° (ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©)
    masks, scores, logits = sam_predictor.predict(
        point_coords=None,
        point_labels=None,
        box=input_box,
        multimask_output=False,
    )
    
    if masks is None or not masks.any():
        print(f"âš ï¸ Warning: No main object mask found for {image_path}. Using full image.")
        mask = np.ones((H, W), dtype=bool)
    else:
        print(f"     -> SAM Mask Score: {scores[0]:.4f}")
        mask = masks[0]
        
    # 2. ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ ì œê±° (ê²€ì€ìƒ‰ìœ¼ë¡œ ì±„ìš°ê¸°)
    # ì´ë¯¸ì§€ ì„ë² ë”©ì€ SAM ëª¨ë¸ì˜ ë‚´ë¶€ ê¸°ëŠ¥(Image Encoder)ì—ì„œ ì¶”ì¶œë˜ë¯€ë¡œ,
    # ë§ˆìŠ¤í¬ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ëŠ” ì €ì¥ ìš©ë„ë¡œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    masked_image_rgb = image_rgb * mask[:, :, np.newaxis]
    
    # ğŸš¨ğŸš¨ğŸš¨ ê°ì²´ë§Œ ë‚¨ê¸´ ì´ë¯¸ì§€ ì €ì¥ ë¡œì§ (í´ë” ì´ë¦„: masked_nonclip) ğŸš¨ğŸš¨ğŸš¨
    save_dir = "masked_nonclip" # ì €ì¥í•  í´ë” ì´ë¦„ ë³€ê²½
    os.makedirs(save_dir, exist_ok=True)
    
    base_name = os.path.basename(image_path)
    file_name_without_ext = os.path.splitext(base_name)[0]
    save_path = os.path.join(save_dir, f"{file_name_without_ext}_masked.png")
    
    # RGBë¥¼ BGRë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (OpenCV ê¸°ë³¸ í¬ë§·)
    masked_image_bgr = cv2.cvtColor(masked_image_rgb, cv2.COLOR_RGB2BGR)
    cv2.imwrite(save_path, masked_image_bgr)
    print(f"     ğŸ’¾ Masked image saved to: {save_path}")
    # ğŸš¨ğŸš¨ğŸš¨ ì €ì¥ ë¡œì§ ë ğŸš¨ğŸš¨ğŸš¨

    # 3. SAM ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ (Maskì™€ ë¬´ê´€í•˜ê²Œ Image Encoderì—ì„œ ì¶”ì¶œ)
    # ì´ ì„ë² ë”©ì€ ì „ì²´ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ì§€ë§Œ, ë§ˆìŠ¤í‚¹ëœ ì´ë¯¸ì§€ì™€ì˜ ìœ ì‚¬ë„ ë¹„êµì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    embedding = sam_predictor.get_image_embedding().cpu().numpy()
    
    return embedding.flatten()


# ==========================
# 3. ì´ë¯¸ì§€ í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ì¶œ
# ==========================
image_dir = "images"
image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)
               if f.lower().endswith((".png", ".jpg", ".jpeg",".webp"))]

if len(image_paths) < 2:
    raise ValueError("âŒ ë¹„êµí•  ì´ë¯¸ì§€ê°€ 2ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤!")

embeddings = []
for path in image_paths:
    print(f"ğŸ”¹ Processing: {path}")
    # ìˆ˜ì •ëœ SAM ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©
    emb = get_sam_embedding_from_masked_object(path)
    embeddings.append(emb)

embeddings = np.array(embeddings).astype("float32")


# ğŸš¨ğŸš¨ğŸš¨ ë²¡í„° ì •ê·œí™” ğŸš¨ğŸš¨ğŸš¨
# L2 ë…¸ë¦„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™”í•´ì•¼ L2 ê±°ë¦¬ë¥¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜ ê°€ëŠ¥
embeddings = normalize(embeddings, axis=1, norm='l2')

print(f"\nâœ… All SAM embeddings extracted. Shape: {embeddings.shape}")


# ==========================
# 4. Faiss ì¸ë±ìŠ¤ ìƒì„± ë° ìœ ì‚¬ë„ ê²€ìƒ‰
# ==========================
dimension = embeddings.shape[1]
# L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤ ì‚¬ìš©
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)
print("âœ… FAISS IndexFlatL2 built.")

# 5. ìœ ì‚¬ë„ ê²€ìƒ‰ (ì˜ˆ: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê¸°ì¤€)
query_idx = 0
query_vector = embeddings[query_idx].reshape(1, -1)

k = 5 # ê²€ìƒ‰í•  ìœ ì‚¬ ì´ë¯¸ì§€ ê°œìˆ˜
distances, indices = index.search(query_vector, k=k) # L2 ê±°ë¦¬ ê²€ìƒ‰

print("\nğŸ” Query Image:", image_paths[query_idx])
print("ğŸ“¸ Most similar images:")

# ğŸš¨ğŸš¨ğŸš¨ ê±°ë¦¬(L2)ë¥¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜ ğŸš¨ğŸš¨ğŸš¨
# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Similarity) = 1 - (L2_Distance^2 / 2)
# ì´ëŠ” ë²¡í„°ê°€ L2 ì •ê·œí™”ë˜ì—ˆì„ ë•Œë§Œ ì„±ë¦½í•©ë‹ˆë‹¤.
cosine_similarities = 1 - (distances ** 2) / 2

for rank, idx in enumerate(indices[0]):
    similarity = cosine_similarities[0][rank]
    
    # ì²« ë²ˆì§¸ ê²°ê³¼ëŠ” ì¿¼ë¦¬ ì´ë¯¸ì§€ ìì‹ 
    if rank == 0:
        print(f"â­ Query itself: {image_paths[idx]} (similarity: {similarity:.4f})")
    else:
        print(f"{rank}. {image_paths[idx]} (similarity: {similarity:.4f})")